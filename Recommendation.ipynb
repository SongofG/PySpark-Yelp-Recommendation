{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Business Recommendation by PySpark and Cosine Similiarity ðŸ¤¤ðŸ˜‹\n",
    "\n",
    "![Yelp Logo from Wikipedia](./images/Yelp_Logo.svg)\n",
    "\n",
    "This project is to create a recommendation engine by leveraging PySpark and Yelp's Open Dataset.\n",
    "\n",
    "Here is the link to the dataset\n",
    "\n",
    "[Yelp data](https://www.yelp.com/dataset)\n",
    "\n",
    "[Yelp documentation for the data](https://www.yelp.com/dataset/documentation/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Data Preprocessing\n",
    "\n",
    "The data is in JSON format.\n",
    "we should first load the data into python dictionary, and get only the fields of interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.functions import col, when\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1.) Loading the Data.\n",
    "\n",
    "In Apache Spark, an **RDD (Resilient Distributed Dataset)** is a fundamental data structure that represents an immutable, distributed collection of objects that can be processed in parallel. RDDs are designed to handle large-scale data processing tasks, offering fault tolerance and data parallelism. They are a key component of Spark and underlie its ability to perform fast, distributed computations.\n",
    "\n",
    "According to the official [Spark Documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html), a rule of thumb of the number of partition is 2 to 4 per CPUs.\n",
    "\n",
    "Here we are going to not specify the number of partitions since Spark automatically decide on the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/23 18:09:39 WARN Utils: Your hostname, Haeins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.0.0.2 instead (on interface en0)\n",
      "24/04/23 18:09:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/23 18:09:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "# Here, \"local[*]\" means to utilize all the cpu available.\n",
    "spark = SparkSession.builder.appName(\"YelpRecommendation\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"Error\")\n",
    "\n",
    "review_rdd = sc.textFile('data/review.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2.) Select the data we are only interested in!\n",
    "\n",
    "First, Let's convert the json into the python dictionaries.\n",
    "\n",
    "And then, we are going to used `user_id`, `business_id`, `stars`, and `date`.\n",
    "\n",
    "Of course we can utilize more columns from the dataset, but let's use the ratings(stars) for this project.\n",
    "\n",
    "In the future, I could expand this project by utilizing the natural language reviews for the user-business pairs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON String into Dictionaries!\n",
    "\n",
    "def json_to_dict(iter):\n",
    "    for x in iter:\n",
    "        yield json.loads(x)\n",
    "\n",
    "review_rdd = review_rdd.mapPartitions(json_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the desired key values\n",
    "def target_columns(iter):\n",
    "    for x in iter:\n",
    "        yield {\n",
    "            'user_id':x['user_id'],\n",
    "            'business_id':x['business_id'],\n",
    "            'stars':x['stars'],\n",
    "            'date':int(x['date'].replace('-','').replace(' ', '').replace(':',''))  # Convert the dates into numerical format for the future deduplication\n",
    "        }\n",
    "\n",
    "review_rdd = review_rdd.mapPartitions(target_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3.) Deduplicate the the records by user, business pair by the date\n",
    "\n",
    "We want to deduplicate the records of the users' review for businesses to the lateset records possible.\n",
    "\n",
    "We do not want a customer's old review on a specific business to affect the recommendation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==============================================>       (161 + 10) / 189]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('JlQ-9fc61X9lbzZN6ZjORQ', 'J3H6VSIgUTlACkb_HPFA8w'): 62\n",
      "('Dl05etq6qzMf5NOskjb7Ag', 'f8IMQgRwo-8GP372MElDGQ'): 38\n",
      "('4d13xAX2jp2EbGF8I9eZZw', 'K_-l_AhoGQvYDeGNgm52qQ'): 36\n",
      "('U6MkHGNWKMVWR8TO31QzeA', 'qNYLt8zizn--wEpT1KILfw'): 32\n",
      "('xq1zRJz5VHgEKiCaI51_Gw', 'z8fuQ6XYwfptZy-Tm1l9aQ'): 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def user_business_count(iter):\n",
    "    for x in iter:\n",
    "        yield ((x['user_id'], x['business_id']), 1)\n",
    "\n",
    "review_rdd_count = review_rdd.mapPartitions(user_business_count)\n",
    "\n",
    "count_by_key = review_rdd_count.reduceByKey(lambda count1, count2: count1 + count2)  # We use reduce because it is more efficient than groupby\n",
    "\n",
    "top_5 = count_by_key.takeOrdered(5, key=lambda x: -x[1])\n",
    "\n",
    "for key, count in top_5:\n",
    "    print(f\"{key}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above code, we see that there are some extreme cases where the same user leaves multiple comments to the same business over a course of time.\n",
    "\n",
    "Therefore, we are going to deduplicate them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, Let's make the date to be the key of the values\n",
    "def user_business_to_key(iter):\n",
    "    for x in iter:\n",
    "        yield ((x['user_id'], x['business_id']), x['date'])  # Here, note that the user, business pair is the key\n",
    "\n",
    "def all_to_key(iter):\n",
    "    for x in iter:\n",
    "        yield ((x[0][0], x[0][1], x[1]), -1)  # ((user_id, business_id, date), -1)\n",
    "\n",
    "review_rdd_date = review_rdd.mapPartitions(user_business_to_key)\n",
    "review_rdd_date = review_rdd_date.reduceByKey(max)  # Deduplicate by the most recent date\n",
    "review_rdd_date = review_rdd_date.mapPartitions(all_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_business_date_to_key(iter):\n",
    "    for x in iter:\n",
    "        yield ((x['user_id'], x['business_id'], x['date']), x['stars'])\n",
    "\n",
    "review_rdd = review_rdd.mapPartitions(user_business_date_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_review_rdd = review_rdd_date.leftOuterJoin(review_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicated Data:\n",
    "\n",
    "```\n",
    "((user_id, bussiness_id, date), (-1, rating))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format back to the dictionary of user_if, business_id, and stars\n",
    "def back_to_dict(iter):\n",
    "    for x in iter:\n",
    "        yield dict(user_id=x[0][0], business_id=x[0][1], stars=x[1][1])\n",
    "\n",
    "recent_review_rdd = recent_review_rdd.mapPartitions(back_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, We filter out the business with less than 30 user reviews\n",
    "def user_id_to_set(iter):\n",
    "    for x in iter:\n",
    "        yield (x['business_id'], set((x['user_id'],)))\n",
    "        \n",
    "# Here, when reduceByKey(), because the user_id is set, we will end up having unique user_ids\n",
    "item_filter_rdd = recent_review_rdd.mapPartitions(user_id_to_set).reduceByKey(lambda x, y: x.union(y)).filter(lambda x: len(x[1]) >= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_and_stars_to_list(iter):\n",
    "    for x in iter:\n",
    "        yield (x['business_id'], [x['user_id'], x['stars']])\n",
    "\n",
    "recent_review_rdd = recent_review_rdd.mapPartitions(user_id_and_stars_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a record from each RDDs look as the following:\n",
    "\n",
    "```\n",
    "item_filter_rdd = (business_id, (user_id1, user_id2, ..., user_idn))\n",
    "\n",
    "recent_review_rdd = (business_id, [user_id, star])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering all the items with less than 30 user reviews\n",
    "thirty_plus_review_rdd = item_filter_rdd.leftOuterJoin(recent_review_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the `leftOuterJoin`, a record from the result RDD looks as below\n",
    "\n",
    "```\n",
    "(business_id, ((user_id1, user_id2, ...), [user_id, stars]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_to_dict(iter):\n",
    "    for x in iter:\n",
    "        yield {'user_id': x[1][1][0], 'business_id':x[0], 'stars':x[1][1][1]}\n",
    "\n",
    "thirty_plus_review_rdd = thirty_plus_review_rdd.mapPartitions(back_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Users with at least 5 distinct items\n",
    "\n",
    "def business_id_to_set(iter):\n",
    "    for x in iter:\n",
    "        yield (x['user_id'], set((x['business_id'],)))\n",
    "        \n",
    "def user_id_and_1_set(iter):\n",
    "    for x in iter:\n",
    "        yield (x[0], 1)\n",
    "\n",
    "user_filter_rdd = thirty_plus_review_rdd.mapPartitions(business_id_to_set).reduceByKey(lambda x, y: x.union(y)).filter(lambda x: len(x[1]) >= 5).mapPartitions(user_id_and_1_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_id_and_stars_to_list(iter):\n",
    "    for x in iter:\n",
    "        yield (x['user_id'], [x['business_id'], x['stars']])\n",
    "\n",
    "thirty_plus_review_rdd = thirty_plus_review_rdd.mapPartitions(business_id_and_stars_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_with_5plus_reviews_rdd = user_filter_rdd.leftOuterJoin(thirty_plus_review_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_to_dict(iter):\n",
    "    for x in iter:\n",
    "        yield {'user_id':x[0], 'business_id':x[1][1][0], 'stars':x[1][1][1]}\n",
    "\n",
    "user_with_5plus_reviews_rdd = user_with_5plus_reviews_rdd.mapPartitions(back_to_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) User-Item Matrix\n",
    "\n",
    "This is a matrix of columns being all the users existing in the dataset, rows being businesses, and each entry being the ratings.\n",
    "\n",
    "For example, let's say there business `A` and `B`, and the data looks like below:\n",
    "\n",
    "|Business|U1|U2|U3|\n",
    "|-----|--|--|--|\n",
    "|A|3|0|1|\n",
    "|B|0|0|5|\n",
    "\n",
    "The above user-item matrix tells us that the business `A` is rated by `U1` and `U3`, but not by `U2`.\n",
    "\n",
    "For business `B`, only `U3` left a review.\n",
    "\n",
    "User `U2` did not leave any reviews on businesses\n",
    "\n",
    "First, we are going to create the user-item matrix!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform into a shape that we can work on the user-item matrix\n",
    "def user_id_and_bussiness_id_to_key(iter):\n",
    "    for x in iter:\n",
    "        yield ((x['user_id'], x['business_id']), x['stars'])\n",
    "\n",
    "user_business_rating_rdd = user_with_5plus_reviews_rdd.mapPartitions(user_id_and_bussiness_id_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('MI3wG3R12sQuTGdNvBBX3A', 'LwQB9H3jZ9wTk24Lr-AnZQ'), 5.0),\n",
       " (('MI3wG3R12sQuTGdNvBBX3A', 'FP-YvQwpYXNPDy-X4AZK9A'), 5.0),\n",
       " (('MI3wG3R12sQuTGdNvBBX3A', 'H16oQAa3G8o3mcqzmGT9Gg'), 4.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_business_rating_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def collect_business_ids(iter):\n",
    "    for x in iter:\n",
    "        yield (x[0][1], 1)\n",
    "\n",
    "def only_business_id(iter):\n",
    "    for x in iter:\n",
    "        yield x[0]\n",
    "\n",
    "def collect_user_ids(iter):\n",
    "    for x in iter:\n",
    "        yield (x[0][0], 1)\n",
    "\n",
    "def only_user_id(iter):\n",
    "    for x in iter:\n",
    "        yield x[0]\n",
    "\n",
    "# Distinct business ids\n",
    "business_id_rdd = user_business_rating_rdd.mapPartitions(collect_business_ids).reduceByKey(lambda x, y: x).mapPartitions(only_business_id)\n",
    "\n",
    "# Distinct user ids\n",
    "user_id_rdd = user_business_rating_rdd.mapPartitions(collect_user_ids).reduceByKey(lambda x, y: x).mapPartitions(only_user_id)\n",
    "\n",
    "\n",
    "business_ids = business_id_rdd.collect()\n",
    "user_ids = user_id_rdd.collect()\n",
    "\n",
    "business_index = sc.broadcast({v: k for k, v in enumerate(business_ids)})\n",
    "user_index = sc.broadcast({v: k for k, v in enumerate(user_ids)})\n",
    "\n",
    "# top_5 = business_id.takeOrdered(5, key=lambda x: -x[1])\n",
    "\n",
    "# for k, v in top_5:\n",
    "#     print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are creating records that has indices rather than the actual user and business names\n",
    "# We are doing this to utilize PySpark's Indexed rows and Indexed row matrix\n",
    "def create_user_business_star_rdd(iter):\n",
    "    for x in iter:\n",
    "        yield (user_index.value[x[0][0]], business_index.value[x[0][1]], x[1])\n",
    "\n",
    "user_business_star_rdd = user_business_rating_rdd.mapPartitions(create_user_business_star_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users_business_rdd = user_business_star_rdd.map(lambda x: (x[0], [(x[1], x[2])])).reduceByKey(lambda x, y: x + y).map(lambda x: {x[0]: x[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_users_business = sc.broadcast(all_users_business_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Calculate the Cosine Similarity!\n",
    "\n",
    "Now, we are going to calculate cosine similarity for each pairs of user rows of the user-item matrix.\n",
    "\n",
    "Here we are going to utilize `IndexedRow`, and `IndexedRowMatrix` from PySpark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def vectorize_ratings(iter):\n",
    "    \n",
    "    # Prepare per-partition dictionaries to collect ratings organized by numeric user index\n",
    "    business_ratings = defaultdict(lambda: defaultdict(float))\n",
    "    for x in iter:\n",
    "        business_ratings[x[1]][x[0]] = x[2]\n",
    "\n",
    "    # Yield IndexedRows using numeric indices and sparse vectors for ratings\n",
    "    for business_idx, ratings in business_ratings.items():\n",
    "\n",
    "        # Sort the dictionary items by keys (business indices) before creating the sparse vector\n",
    "        sorted_items = sorted(ratings.items())\n",
    "        indices = [item[0] for item in sorted_items]\n",
    "        values = [item[1] for item in sorted_items]\n",
    "        \n",
    "        yield IndexedRow(business_idx, Vectors.sparse(len(user_index.value), indices, values))\n",
    "\n",
    "indexed_row_rdd = user_business_star_rdd.mapPartitions(vectorize_ratings)\n",
    "\n",
    "indexed_row_matrix = IndexedRowMatrix(indexed_row_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the similarity of users!\n",
    "similarity_matrix = indexed_row_matrix.columnSimilarities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Recommendation for the Target Users!\n",
    "\n",
    "Finally! now we give recommendation to the target users.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selected target users\n",
    "\n",
    "target_users = ['PomQayG1WhMxeSl1zohAUA', 'uEvusDwoSymbJJ0auR3muQ', 'q6XnQNNOEgvZaeizUgHTSw', 'n00anwqzOaR52zgMRaZLVQ', 'qOdmye8UQdqloVNE059PkQ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the user_id into user_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[195902, 37701, 33240, 15189, 2773]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_users_id = list()\n",
    "\n",
    "for user_id in target_users:\n",
    "    target_users_id.append(user_index.value[user_id])\n",
    "\n",
    "target_users_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let's convert the similarity matrix into a dataframe\n",
    "similarity_entries_df = spark.createDataFrame(similarity_matrix.entries)\n",
    "\n",
    "# Get the businesses of the similar users\n",
    "def get_businesses_of_similar_users(iter):\n",
    "    for x in iter:\n",
    "        yield x[1]\n",
    "\n",
    "# get the top five similar users!\n",
    "def get_top_similar_users(target_user_id, num_similar_users=3):\n",
    "    # Filter rows where the current user_id appears in either the 'i' or 'j' columns\n",
    "    return similarity_entries_df.filter(\n",
    "        (col(\"i\") == target_user_id) | (col(\"j\") == target_user_id)\n",
    "    ).withColumn(\n",
    "        \"other_user_id\",\n",
    "        when(col(\"i\") == target_user_id, col(\"j\")).otherwise(col(\"i\"))\n",
    "    ).withColumn(\n",
    "        \"similarity_score\",\n",
    "        col(\"value\")  # Directly use the 'value' column which contains similarity scores\n",
    "    ).orderBy(\n",
    "        col(\"value\").desc()  # Order by the similarity score in descending order\n",
    "    ).limit(\n",
    "        num_similar_users  # Limit to the top 'num_similar_users' similar users\n",
    "    ).select(\n",
    "        \"other_user_id\", \"similarity_score\"  # Select both the user ID and similarity score\n",
    "    )\n",
    "\n",
    "\n",
    "# RECOMMENDATION!!\n",
    "def recommend_business(target_user_id):\n",
    "\n",
    "    # Get the target user's dictionary\n",
    "    for target in all_users_business.value:\n",
    "        if target_user_id in target:\n",
    "            target_user_dict = target\n",
    "            break\n",
    "\n",
    "    # Fetch the user's existing ratings from the broadcast variable\n",
    "    user_data = target_user_dict.get(target_user_id, [])\n",
    "    \n",
    "    # Extract already rated business IDs\n",
    "    already_rated_business_ids = set([business for business, _ in user_data])\n",
    "\n",
    "    # Fetch top similar users\n",
    "    top_similar_users = get_top_similar_users(target_user_id).collect()\n",
    "    similar_user_ids = [row['other_user_id'] for row in top_similar_users]\n",
    "\n",
    "    # Fetch and aggregate businesses rated above 4 by these similar users\n",
    "    similar_users_businesses = user_business_star_rdd.filter(\n",
    "        lambda x: x[0] in similar_user_ids and x[2] >= 4 and x[1] not in already_rated_business_ids\n",
    "    ).map(lambda x: x[1]).distinct().collect()\n",
    "\n",
    "    # Filter and recommend businesses not yet rated by the target user\n",
    "    recommendations = [bus_id for bus_id in similar_users_businesses if bus_id not in already_rated_business_ids]\n",
    "\n",
    "    return recommendations, similar_users_businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:==================================================>(1483 + 10) / 1512]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 37701: [46901, 22775, 48618, 6604, 13510]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "target_user_id = 37701  # This user ID should be from your broadcast variable\n",
    "recommendations, _ = recommend_business(target_user_id)\n",
    "print(f\"Recommendations for user {target_user_id}: {recommendations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DefaultKernel_python3.10.11",
   "language": "python",
   "name": "defaultkernel_python3.10.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
